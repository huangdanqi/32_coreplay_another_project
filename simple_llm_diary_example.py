#!/usr/bin/env python3
"""
Simple LLM Diary Generation - Working Examples
Shows how to use LLM to generate diary entries effectively
"""

import asyncio
import sys
import os
from datetime import datetime

# Add the diary_agent directory to the path
sys.path.append(os.path.join(os.path.dirname(__file__), 'diary_agent'))

from diary_agent.core.llm_manager import LLMConfigManager


async def simple_llm_diary_generation():
    """Simple LLM diary generation examples"""
    
    print("ğŸ¤– Simple LLM Diary Generation")
    print("=" * 50)
    
    # Initialize LLM manager (uses your JSON config)
    llm_manager = LLMConfigManager()
    
    print(f"âœ“ Using LLM Provider: {llm_manager.get_default_provider()}")
    print(f"âœ“ Model: qwen3:4b (via Ollama)")
    
    # Example 1: Simple diary prompt
    print(f"\nğŸ“ Example 1: Simple Diary Entry")
    simple_prompt = """è¯·ä¸ºç©å…·å†™ä¸€ç¯‡ç®€çŸ­çš„æ—¥è®°ï¼š

ä»Šå¤©ç©å…·çš„æœ‹å‹å°çŒ«å’ªæ¥å®¶é‡Œåšå®¢äº†ï¼Œç©å…·å¾ˆå¼€å¿ƒã€‚

è¦æ±‚ï¼š
- æ ‡é¢˜ï¼šæœ€å¤š6ä¸ªå­—ç¬¦
- å†…å®¹ï¼šæœ€å¤š35ä¸ªå­—ç¬¦ï¼Œå¯ä»¥åŒ…å«è¡¨æƒ…ç¬¦å·
- æƒ…æ„Ÿï¼šå¼€å¿ƒå¿«ä¹"""
    
    try:
        result = await llm_manager.generate_text_with_failover(
            prompt=simple_prompt,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“é—¨å†™ç©å…·æ—¥è®°çš„åŠ©æ‰‹ã€‚"
        )
        print(f"  LLM Response: {result}")
    except Exception as e:
        print(f"  Error: {e}")
    
    # Example 2: Weather diary
    print(f"\nğŸ“ Example 2: Weather Diary")
    weather_prompt = """è¯·ä¸ºç©å…·å†™ä¸€ç¯‡å…³äºå¤©æ°”çš„æ—¥è®°ï¼š

ä»Šå¤©ä¸‹é›¨äº†ï¼Œç©å…·ä¸èƒ½å‡ºå»ç©ï¼Œæœ‰ç‚¹ä¸å¼€å¿ƒã€‚

è¦æ±‚ï¼š
- æ ‡é¢˜ï¼šæœ€å¤š6ä¸ªå­—ç¬¦
- å†…å®¹ï¼šæœ€å¤š35ä¸ªå­—ç¬¦ï¼Œå¯ä»¥åŒ…å«è¡¨æƒ…ç¬¦å·
- æƒ…æ„Ÿï¼šè¡¨è¾¾ä¸å¼€å¿ƒ"""
    
    try:
        result = await llm_manager.generate_text_with_failover(
            prompt=weather_prompt,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“é—¨å†™ç©å…·æ—¥è®°çš„åŠ©æ‰‹ã€‚"
        )
        print(f"  LLM Response: {result}")
    except Exception as e:
        print(f"  Error: {e}")
    
    # Example 3: Owner interaction diary
    print(f"\nğŸ“ Example 3: Owner Interaction Diary")
    owner_prompt = """è¯·ä¸ºç©å…·å†™ä¸€ç¯‡å…³äºä¸»äººäº’åŠ¨çš„æ—¥è®°ï¼š

ä¸»äººç»™ç©å…·ä¹°äº†æ–°ç©å…·ï¼Œç©å…·å¾ˆå…´å¥‹ã€‚

è¦æ±‚ï¼š
- æ ‡é¢˜ï¼šæœ€å¤š6ä¸ªå­—ç¬¦
- å†…å®¹ï¼šæœ€å¤š35ä¸ªå­—ç¬¦ï¼Œå¯ä»¥åŒ…å«è¡¨æƒ…ç¬¦å·
- æƒ…æ„Ÿï¼šå…´å¥‹æ¿€åŠ¨"""
    
    try:
        result = await llm_manager.generate_text_with_failover(
            prompt=owner_prompt,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“é—¨å†™ç©å…·æ—¥è®°çš„åŠ©æ‰‹ã€‚"
        )
        print(f"  LLM Response: {result}")
    except Exception as e:
        print(f"  Error: {e}")


async def structured_diary_generation():
    """Generate structured diary entries"""
    
    print(f"\nğŸ¯ Structured Diary Generation")
    print("=" * 50)
    
    # Initialize LLM manager
    llm_manager = LLMConfigManager()
    
    # Structured prompt for better results
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“é—¨å†™ç©å…·æ—¥è®°çš„åŠ©æ‰‹ã€‚
è¯·æ ¹æ®äº‹ä»¶ç”Ÿæˆç®€çŸ­çš„æ—¥è®°æ¡ç›®ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
æ ‡é¢˜ï¼š[6ä¸ªå­—ç¬¦ä»¥å†…çš„æ ‡é¢˜]
å†…å®¹ï¼š[35ä¸ªå­—ç¬¦ä»¥å†…çš„å†…å®¹ï¼Œå¯åŒ…å«è¡¨æƒ…ç¬¦å·]
æƒ…æ„Ÿï¼š[æƒ…æ„Ÿæè¿°]"""
    
    # Different event types
    events = [
        {
            "name": "æ–°æœ‹å‹äº‹ä»¶",
            "description": "ç©å…·è®¤è¯†äº†æ–°æœ‹å‹å°å…”å­ï¼Œå¾ˆå¼€å¿ƒ",
            "expected_emotion": "å¼€å¿ƒå¿«ä¹"
        },
        {
            "name": "å¤©æ°”äº‹ä»¶", 
            "description": "ä»Šå¤©ä¸‹é›¨äº†ï¼Œç©å…·ä¸èƒ½å‡ºå»ç©ï¼Œæœ‰ç‚¹å¤±è½",
            "expected_emotion": "ä¸å¼€å¿ƒ"
        },
        {
            "name": "ä¸»äººäº’åŠ¨",
            "description": "ä¸»äººé™ªç©å…·ç©äº†å¾ˆä¹…ï¼Œç©å…·å¾ˆæ»¡è¶³",
            "expected_emotion": "æ»¡è¶³"
        }
    ]
    
    for event in events:
        print(f"\nğŸ“ {event['name']}")
        
        user_prompt = f"""è¯·ä¸ºä»¥ä¸‹äº‹ä»¶å†™æ—¥è®°ï¼š

äº‹ä»¶ï¼š{event['description']}
æœŸæœ›æƒ…æ„Ÿï¼š{event['expected_emotion']}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›å¤ï¼š
æ ‡é¢˜ï¼š[æ ‡é¢˜]
å†…å®¹ï¼š[å†…å®¹]
æƒ…æ„Ÿï¼š[æƒ…æ„Ÿ]"""
        
        try:
            result = await llm_manager.generate_text_with_failover(
                prompt=user_prompt,
                system_prompt=system_prompt
            )
            print(f"  LLM Response: {result}")
        except Exception as e:
            print(f"  Error: {e}")


async def main():
    """Main function to demonstrate LLM diary generation"""
    print("ğŸš€ LLM Diary Generation Examples")
    
    # Test 1: Simple diary generation
    await simple_llm_diary_generation()
    
    # Test 2: Structured diary generation
    await structured_diary_generation()
    
    print(f"\nâœ… LLM Diary Generation Complete!")
    print(f"\nğŸ’¡ Tips:")
    print(f"  - Your JSON config is working perfectly!")
    print(f"  - LLM is generating diary entries successfully")
    print(f"  - Use simple prompts for best results")
    print(f"  - The specialized agents work better than direct LLM calls")


if __name__ == "__main__":
    asyncio.run(main())
