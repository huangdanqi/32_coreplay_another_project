"""
Test script to verify Ollama connection and Qwen3 model availability.
Run this to make sure your local Ollama setup is working before running the diary agent.
"""

import asyncio
import aiohttp
import json


async def test_ollama_connection():
    """Test if Ollama is running and Qwen3 model is available."""
    
    print("ğŸ” Testing Ollama Connection...")
    
    # Test 1: Check if Ollama is running
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get("http://localhost:11434/api/tags") as response:
                if response.status == 200:
                    models = await response.json()
                    print("âœ… Ollama is running!")
                    print(f"ğŸ“‹ Available models: {len(models.get('models', []))}")
                    
                    # Check if qwen3 is available
                    model_names = [model['name'] for model in models.get('models', [])]
                    print(f"ğŸ” Models found: {model_names}")
                    
                    if any('qwen3' in name.lower() for name in model_names):
                        print("âœ… Qwen3 model found!")
                    else:
                        print("âŒ Qwen3 model not found. Available models:")
                        for model in models.get('models', []):
                            print(f"   - {model['name']}")
                        print("\nğŸ’¡ To install Qwen3, run: ollama pull qwen3")
                        return False
                else:
                    print(f"âŒ Ollama API returned status {response.status}")
                    return False
                    
    except aiohttp.ClientConnectorError:
        print("âŒ Cannot connect to Ollama. Is it running?")
        print("ğŸ’¡ Start Ollama with: ollama serve")
        return False
    except Exception as e:
        print(f"âŒ Error testing Ollama: {e}")
        return False
    
    # Test 2: Try generating text with Qwen3
    print("\nğŸ§ª Testing text generation with Qwen3...")
    
    try:
        payload = {
            "model": "qwen3:4b",
            "prompt": "ä½ å¥½ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚è¯·å†™ä¸€å¥ç®€çŸ­çš„æ—¥è®°ã€‚",
            "options": {
                "num_predict": 50,
                "temperature": 0.7
            },
            "stream": False
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                "http://localhost:11434/api/generate",
                headers={"Content-Type": "application/json"},
                json=payload
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    generated_text = result.get("response", "").strip()
                    print(f"âœ… Text generation successful!")
                    print(f"ğŸ“ Generated: {generated_text}")
                    return True
                else:
                    error_text = await response.text()
                    print(f"âŒ Text generation failed: {response.status}")
                    print(f"Error: {error_text}")
                    return False
                    
    except Exception as e:
        print(f"âŒ Error during text generation: {e}")
        return False


async def test_diary_agent_llm_config():
    """Test the diary agent LLM configuration."""
    
    print("\nğŸ”§ Testing Diary Agent LLM Configuration...")
    
    try:
        from diary_agent.core.llm_manager import LLMConfigManager
        
        # Initialize LLM manager
        llm_manager = LLMConfigManager()
        
        # Check if ollama_qwen3 provider is configured
        ollama_config = llm_manager.get_provider_config("ollama_qwen3")
        if ollama_config:
            print("âœ… Ollama Qwen3 provider configured!")
            print(f"   Endpoint: {ollama_config.api_endpoint}")
            print(f"   Model: {ollama_config.model_name}")
        else:
            print("âŒ Ollama Qwen3 provider not found in configuration")
            return False
        
        # Test text generation through LLM manager
        print("\nğŸ§ª Testing text generation through LLM manager...")
        
        test_prompt = "ä»Šå¤©æ˜¯æ™´å¤©ï¼Œå¿ƒæƒ…å¾ˆå¥½ã€‚è¯·å†™ä¸€å¥ç®€çŸ­çš„æ—¥è®°ã€‚"
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªæ—¥è®°åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å†™ç®€çŸ­çš„æ—¥è®°å†…å®¹ï¼Œä¸è¶…è¿‡35ä¸ªå­—ç¬¦ã€‚"
        
        generated_text = await llm_manager.generate_text_with_failover(
            prompt=test_prompt,
            system_prompt=system_prompt
        )
        
        print(f"âœ… LLM Manager text generation successful!")
        print(f"ğŸ“ Generated: {generated_text}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error testing LLM manager: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Run all tests."""
    
    print("ğŸ¯ OLLAMA CONNECTION TEST")
    print("=" * 50)
    
    # Test Ollama connection
    ollama_ok = await test_ollama_connection()
    
    if ollama_ok:
        # Test diary agent configuration
        config_ok = await test_diary_agent_llm_config()
        
        if config_ok:
            print("\n" + "=" * 50)
            print("ğŸ‰ ALL TESTS PASSED!")
            print("âœ… Ollama is working")
            print("âœ… Qwen3 model is available") 
            print("âœ… Diary agent LLM configuration is correct")
            print("\nğŸš€ You can now run: python diary_agent_api_example.py")
        else:
            print("\nâŒ LLM configuration test failed")
    else:
        print("\nâŒ Ollama connection test failed")
        print("\nğŸ”§ Setup steps:")
        print("1. Install Ollama: https://ollama.ai/")
        print("2. Start Ollama: ollama serve")
        print("3. Install Qwen3: ollama pull qwen3")


if __name__ == "__main__":
    asyncio.run(main())